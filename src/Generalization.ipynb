{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test networks with Gym model with additional steps\n",
    "\n",
    "use trained policy (trained with original weight = 10 kg) to run pendulum model with weight = 5 kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out,size_hidden):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\tself.layer1 = nn.Linear(size_in, size_hidden)\n",
    "\t\tself.layer2 = nn.Linear(size_hidden, size_hidden)\n",
    "\t\tself.layer3 = nn.Linear(size_hidden, size_out)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float)\n",
    "\t\tm = nn.Tanh()\n",
    "\t\tactivation1 = F.relu(self.layer1(obs.float()))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\t\toutput = 2 * m(output)\n",
    "\t\treturn output.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_gen(seed = 0, profix = \"\", timesteps_per_batch = 5000, name = \"\", m = None):\n",
    "    ''' hyperparameter '''\n",
    "    # collect data\n",
    "    timesteps_per_batch = timesteps_per_batch                   # Number of timesteps to run per batch, episode\n",
    "    max_timesteps_per_episode = 200                             # Max number of timesteps per episode, steps\n",
    "\n",
    "    total_timesteps = 200                                       # collect how many times\n",
    "    n_updates_per_iteration = 1                                 # Number of times to update actor/critic per iteration\n",
    "    \n",
    "\n",
    "    lr = 0.005                                 # Learning rate of actor optimizer\n",
    "    gamma = 0.95                               # Discount factor to be applied when calculating Rewards-To-Go\n",
    "    clip = 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "    \n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    if m:   # m is the weight in model\n",
    "        env.m = m\n",
    "\n",
    "    filename = name + f\"_{timesteps_per_batch}_gen\"\n",
    "    if m:\n",
    "        filename += f\"_m{m}\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # set random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    ''' init networks '''\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]        # 1\n",
    "    \n",
    "    # actor = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    # critic = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    actor = torch.load(name + \"_actor.pth\")\n",
    "    critic = torch.load(name + \"_critic.pth\")\n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "    cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "\n",
    "    all_average_reward = []\n",
    "    all_variance = []\n",
    "\n",
    "    ''' methods '''\n",
    "    def rollout():  # collect data for episode times, each episode contains n steps \n",
    "        # Batch data. For more details, check function header.\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "        batch_reward_average = []\n",
    "        reward_all = []\n",
    "\n",
    "        ep_rews = []\n",
    "\n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "\n",
    "        # Keep simulating until we've run more than or equal to specified timesteps per batch\n",
    "        while t < timesteps_per_batch:      # like episode\n",
    "            ep_rews = [] # rewards collected per episode\n",
    "\n",
    "            # Reset the environment. sNote that obs is short for observation. \n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Run an episode for a maximum of max_timesteps_per_episode timesteps\n",
    "            for ep_t in range(max_timesteps_per_episode):       # like steps\n",
    "                t += 1 # Increment timesteps ran this batch so far\n",
    "\n",
    "                # Track observations in this batch\n",
    "                batch_obs.append(obs)\n",
    "\n",
    "                # Calculate action and make a step in the env. \n",
    "                # Note that rew is short for reward.\n",
    "                action, log_prob = get_action(obs)\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "\n",
    "                # Track recent reward, action, and action log probability\n",
    "                ep_rews.append(rew)\n",
    "                reward_all.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                # If the environment tells us the episode is terminated, break\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Track episodic lengths and rewards\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_reward_average.append(sum(ep_rews)/len(ep_rews))\n",
    "        batch_obs = torch.from_numpy(np.array(batch_obs))#torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.from_numpy(np.array(batch_acts))#torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.from_numpy(np.array(batch_log_probs))#torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "\n",
    "        batch_rtgs = compute_rtgs(batch_rews)                                                              # ALG STEP 4\n",
    "        r_all_average = sum(reward_all) / len(reward_all)\n",
    "        all_variance.append(torch.tensor(batch_rews).var().item())\n",
    "\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average\n",
    "\n",
    "    def compute_rtgs(batch_rews):\n",
    "        # The rewards-to-go (rtg) per episode per batch to return.\n",
    "        # The shape will be (num timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "\n",
    "        # Iterate through each episode\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "\n",
    "            # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "            # discounted return (think about why it would be harder starting from the beginning)\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        # Convert the rewards-to-go into a tensor\n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "        return batch_rtgs\n",
    "\n",
    "    def get_action(obs):\n",
    "        # Query the actor network for a mean action\n",
    "        mean = actor(obs)\n",
    "\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "        # Sample an action from the distribution\n",
    "        # action = dist.sample()    # TODO: changed\n",
    "        action = dist.rsample()\n",
    "\n",
    "        # Calculate the log probability for that action\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        # Return the sampled action and the log probability of that action in our distribution\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "\n",
    "    def evaluate(batch_obs, batch_acts):\n",
    "        # Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "        V = critic(batch_obs).squeeze()\n",
    "        # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "        # This segment of code is similar to that in get_action()\n",
    "        mean = actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        # Return the value vector V of each observation in the batch\n",
    "        # and log probabilities log_probs of each action in the batch\n",
    "        return V, log_probs\n",
    "\n",
    "    print(f\"Learning... Running {max_timesteps_per_episode} timesteps per episode, \", end='')\n",
    "    print(f\"{timesteps_per_batch} timesteps per batch for a total of {total_timesteps} timesteps\")\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    i_so_far = 0 # Iterations ran so far\n",
    "    while t_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
    "        # Autobots, roll out (just kidding, we're collecting our batch simulations here)\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average = rollout()                     # ALG STEP 3\n",
    "\n",
    "        # all_average_reward.append(batch_rtgs.mean().item())\n",
    "        all_average_reward.append(r_all_average) \n",
    "\n",
    "        # Calculate how many timesteps we collected this batch\n",
    "        # t_so_far += np.sum(batch_lens)\n",
    "        t_so_far += 1\n",
    "\n",
    "        # Increment the number of iterations\n",
    "        i_so_far += 1\n",
    "\n",
    "        # Calculate advantage at k-th iteration, V from critic, rtgs from actor\n",
    "        V, _ = evaluate(batch_obs, batch_acts)\n",
    "        A_k = batch_rtgs - V.detach()                                                                       # ALG STEP 5\n",
    "\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "        # This is the loop where we update our network for some n epochs\n",
    "        ''' update n times'''\n",
    "        for _ in range(n_updates_per_iteration):                                                       # ALG STEP 6 & 7\n",
    "            # Calculate V_phi and pi_theta(a_t | s_t)\n",
    "            V, curr_log_probs = evaluate(batch_obs, batch_acts)\n",
    "\n",
    "            # Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
    "            ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "            # Calculate surrogate losses.\n",
    "            surr1 = ratios * A_k\n",
    "            surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "\n",
    "            # Calculate actor and critic losses.\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for actor network\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optim.step()\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for critic network\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "\n",
    "        ''' show result '''\n",
    "        if t_so_far % 100 == 0:\n",
    "            # print(f\"Episode: {t_so_far}/{total_timesteps}, average:{average_score[-1]}\")\n",
    "            print(f\"Episode: {t_so_far}/{total_timesteps}, average reward: {all_average_reward[-1]}\")\n",
    "    ''' save result '''\n",
    "    plt.figure(1)\n",
    "    x = range(1,len(all_average_reward)+1)\n",
    "\n",
    "    plt.plot(x,all_average_reward,label = 'average rewards')\n",
    "    plt.axhline(y=sum(all_average_reward)/len(all_average_reward),c='r', ls=\"--\")\n",
    "    plt.axhline(y=0,c='g', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-9, 1))\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename + '_collect_rewards' + '.jpg')\n",
    "    plt.show()\n",
    "    print(filename)\n",
    "    torch.save(actor, filename+'_actor' + '.pth')  \n",
    "    torch.save(critic, filename+'_critic' + '.pth')  \n",
    "    print(\"---\")\n",
    "\n",
    "    plt.plot(x,all_variance,label = 'all_variance')\n",
    "    plt.axhline(y=sum(all_variance)/len(all_variance),c='r', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.title(filename + \"_var\")\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'_var.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    ''' save data in file '''\n",
    "    reward_name = filename + \"-rew.csv\"\n",
    "    file = open(reward_name, 'w')\n",
    "    ### s = \";\".join([str(x) for x in average_score])\n",
    "    s = \"\\n\".join([str(x) for x in all_average_reward])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    var_name = filename + \"-var.csv\"\n",
    "    file = open(var_name, 'w')\n",
    "    # file.write(\"varience\\n\")    # save time\n",
    "    b = \"\\n\".join([str(x) for x in all_variance])\n",
    "    file.write(b)\n",
    "\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policynames = [\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[0]_0412-1244\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[1]_0412-1457\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[2]_0412-1716\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[3]_0412-1919\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[4]_0412-2115\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[5]_0412-2309\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[6]_0413-1144\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[7]_0413-1429\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[8]_0413-1715\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[9]_0413-1954\",\n",
    "\"PPOtanh__md2_total[200]_up[1]_ep[5000]_step[200]_lr[0.005]_cp[0.2]_sd[10]_0413-2211\",\n",
    "\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[0]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[1]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[2]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[3]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[4]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[5]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[6]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[7]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[8]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[9]\",\n",
    "\"train-3000/PPOtanh__md2_total[200]_up[1]_ep[3000]_step[200]_lr[0.005]_cp[0.2]_sd[10]\"]\n",
    "\n",
    "for name in policynames:\n",
    "    PPO_gen(seed=0, profix=\"r-tanh\", timesteps_per_batch = 2000, name = name)\n",
    "    # try weight = 5 kg\n",
    "    PPO_gen(seed=0, profix=\"r-tanh\", timesteps_per_batch = 2000, name = name, m = 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc5af479086f4608decbb100b98c7101a70c8924355d1bcefce5624369fc4f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
