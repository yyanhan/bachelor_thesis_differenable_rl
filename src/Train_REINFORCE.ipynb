{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train REINFOECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state value baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValue(nn.Module):\n",
    "    def __init__(self,s_size=3, h_size=64, out_size = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(s_size, h_size)\n",
    "        self.output_layer = nn.Linear(h_size, out_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #input layer\n",
    "        # print(state)\n",
    "        x = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # print(x)\n",
    "        x = self.input_layer(x)\n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "        #get state value\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        ''' not hardtanh'''\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi\n",
    "\n",
    "def angle_normalize_np(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy for continous action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out,size_hidden):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\tself.layer1 = nn.Linear(size_in, size_hidden)\n",
    "\t\tself.layer2 = nn.Linear(size_hidden, size_out)\n",
    "\t\t# self.layer2 = nn.Linear(size_hidden, size_hidden)\n",
    "\t\t# self.layer3 = nn.Linear(size_hidden, size_out)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "\t\tm = nn.Tanh()\n",
    "\t\tactivation1 = F.relu(self.layer1(obs))\n",
    "\t\toutput = 2 * m(self.layer2(activation1))\n",
    "\t\t# activation2 = F.relu(self.layer2(activation1))\n",
    "\t\t# output = self.layer3(activation2)\n",
    "\t\t# output = 2 * m(activation2)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the main training algorithm for REINFORCE main part with Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "def conti(ns, n_episodes = 5000, state_value = False, white = False, number = 0, lr_n = 2, lr_baseline = 1, h_size_policy = 64, h_size_baseline = 64,\\\n",
    "    learning_rate_policy = None, learning_rate_baseline = None):\n",
    "    \"\"\" main training part for REINFORCE\n",
    "\n",
    "    Args:\n",
    "        ns (int): number of random seed\n",
    "        n_episodes (int, optional): episodes. Defaults to 5000.\n",
    "        state_value (bool, optional): if use baseline technique state value. Defaults to False.\n",
    "        white (bool, optional): if use whitening technique. Defaults to False.\n",
    "        number (int, optional): the number of training, only for distinguish filename. Defaults to 0.\n",
    "        lr_n (int, optional): learning rate of optimizer 1e-n. Defaults to 2.\n",
    "        lr_baseline (int, optional): learning rate of baseline statevalue, 1e-n. Defaults to 1.\n",
    "        h_size_policy (int, optional): size of hidden layer of policy. Defaults to 64.\n",
    "        h_size_baseline (int, optional): size of hidden layer of baseline statevalue. Defaults to 64.\n",
    "        learning_rate_policy (_type_, optional): specfic learning rate for policy, not 1e-n, but the whole value. Defaults to None.\n",
    "        learning_rate_baseline (_type_, optional): specfic learning rate for baseline state value, not 1e-n, but the whole value. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    ''' configuration '''\n",
    "    time_1 = time.time()\n",
    "    n_episodes = n_episodes\n",
    "    max_t = 200\n",
    "    gamma = 0.9\n",
    "    h_size = h_size_policy\n",
    "\n",
    "    lr = 10**(-lr_n)\n",
    "    conti_baseline_lr = 10**(-lr_baseline)\n",
    "\n",
    "    if learning_rate_policy != None:\n",
    "        lr = learning_rate_policy\n",
    "        lr_n = learning_rate_policy\n",
    "    if learning_rate_baseline != None:\n",
    "        conti_baseline_lr = learning_rate_baseline\n",
    "        lr_baseline = learning_rate_baseline\n",
    "\n",
    "    np.random.seed(ns)\n",
    "    torch.manual_seed(ns) # set random seed\n",
    "    random.seed(ns)\n",
    "\n",
    "    conti_state_value_hsize = h_size_baseline\n",
    "    # conti_baseline_lr = 1e-1\n",
    "\n",
    "    # isContiStateValue = True\n",
    "    # isContiStateValue = False\n",
    "\n",
    "    isContiStateValue = state_value\n",
    "\n",
    "    whitening = white\n",
    "    # whitening = True\n",
    "    # whitening = False\n",
    "    \n",
    "\n",
    "    # filename = f\"{n_episodes}-{max_t}-{lr_n}-hs{h_size}-g{str(gamma*10)}\"\n",
    "    # filename = \"Gym-ori-\" + filename\n",
    "    filename = \"Gym-Reinforce-ori_model-\"\n",
    "    if isContiStateValue:\n",
    "        filename = filename + f'-baseline-hs{conti_state_value_hsize}-svlr{lr_baseline}'\n",
    "    \n",
    "    if whitening:\n",
    "        filename = filename + \"-w\"\n",
    "\n",
    "    # filename = filename + \"-\" + f\"sd{ns}-\" + str(number)\n",
    "    filename = filename + \"-\" + f\"sd{ns}\"\n",
    "    print(filename)\n",
    "\n",
    "    ''' init envionment, policy '''\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    env.seed(ns)\n",
    "    # policy = Policy_conti(hidden_size=h_size,num_inputs=3,action_space=env.action_space)\n",
    "    policy = Policy_conti_np(hidden_size=h_size,num_inputs=3,action_space=env.action_space)\n",
    "    # policy = Network(size_hidden=h_size,size_in=3,size_out=env.action_space)\n",
    "    \n",
    "    if isContiStateValue:    # baseline\n",
    "        conti_state_value_policy = StateValue(h_size=conti_state_value_hsize)\n",
    "        optimizer_state_value = optim.Adam(conti_state_value_policy.parameters(), lr=conti_baseline_lr)\n",
    "\n",
    "\n",
    "    average_score = []\n",
    "    average_state_value = []\n",
    "    all_variance = []\n",
    "\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    ''' training process'''\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        # print(f\"----- [{i_episode}] -----\")\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_rewards_original = []\n",
    "        episode_entropies = []\n",
    "        conti_episode_state_values = []\n",
    "        state = env.reset()   \n",
    "        ''' start each step'''\n",
    "        for t in range(max_t):\n",
    "            action, log_prob, entropy = policy(state)\n",
    "            \n",
    "            if isContiStateValue:   # baseline\n",
    "                state_value = conti_state_value_policy(state)       # baseline\n",
    "                conti_episode_state_values.append(state_value)      # baseline\n",
    "            state, reward, done, _ = env.step(action.detach().numpy()[0])\n",
    "            # state, reward, done, _ = env.step(action.numpy())\n",
    "\n",
    "            episode_entropies.append(entropy)\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "            episode_rewards_original.append(reward)\n",
    "            ''' end each step '''\n",
    "\n",
    "        ''' save average value, before operation'''\n",
    "        average_score.append(sum(episode_rewards_original).item()/max_t)\n",
    "        if isContiStateValue:\n",
    "            average_state_value.append(sum(conti_episode_state_values).item()/len(conti_episode_state_values))\n",
    "            sv = torch.FloatTensor(conti_episode_state_values)\n",
    "\n",
    "        ''' process reward with whitening '''\n",
    "        if whitening:\n",
    "            episode_rewards = torch.FloatTensor(episode_rewards)\n",
    "            episode_rewards.requires_grad = True\n",
    "            all_variance.append(episode_rewards.var().item())\n",
    "            episode_rewards = (episode_rewards - episode_rewards.mean())/episode_rewards.std()\n",
    "\n",
    "        ''' update policy '''\n",
    "        R = torch.zeros(1, 1)\n",
    "        baseline = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        state_value_delta = []\n",
    "        R_list = []\n",
    "\n",
    "        for i in reversed(range(len(episode_rewards))):\n",
    "            R = gamma * R + episode_rewards[i]  # R is value-function of each step\n",
    "            # baseline = gamma * baseline + sv[i]\n",
    "            if isContiStateValue:\n",
    "                # r = R - conti_episode_state_values[i]\n",
    "                # r = R - baseline\n",
    "                r = R - sv[i] # r is (value-function) - (state value function)\n",
    "                state_value_delta.append(r) # used for update policy\n",
    "                R_list.append(R)        # from the last to the first, need reversed later, used for update the state-value function\n",
    "                ''' update policy '''\n",
    "                loss = loss - (episode_log_probs[i]*(r.expand_as(episode_log_probs[i]))).sum() - (0.0001*episode_entropies[i]).sum()\n",
    "\n",
    "                # ''' nop '''\n",
    "                # loss = loss - ((r.expand_as(episode_log_probs[i]))).sum() - (0.0001*episode_entropies[i]).sum()\n",
    "            else:   # if not use baseline: only use R\n",
    "                loss = loss - (episode_log_probs[i]*(R.expand_as(episode_log_probs[i]))).sum() - (0.0001*episode_entropies[i]).sum()\n",
    "                # loss = loss - (episode_log_probs[i]*(R)).sum() - (0.0001*episode_entropies[i]).sum()\n",
    "                # ''' nop '''\n",
    "                # loss = loss - ((R.expand_as(episode_log_probs[i]))).sum() - (0.0001*episode_entropies[i]).sum()\n",
    "\n",
    "        loss = loss / len(episode_rewards)\n",
    "        # print(R_list)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % 1000 == 0:\n",
    "            # print(f\"Episode: {i_episode}/{n_episodes}, average:{average_score[-1]}, average_state_value: {average_state_value[-1]}\")\n",
    "            \n",
    "            if isContiStateValue:\n",
    "                print(f\"Episode: {i_episode}/{n_episodes}, average:{average_score[-1]}, average_state_value: {average_state_value[-1]}\")\n",
    "            else:\n",
    "                print(f\"Episode: {i_episode}/{n_episodes}, average:{average_score[-1]}\")\n",
    "\n",
    "    ''' show results '''\n",
    "    time_2 = time.time()\n",
    "    x = range(1,len(average_score)+1)\n",
    "    \n",
    "    if isContiStateValue:\n",
    "        plt.plot(x,average_state_value,label = 'average state value',c='r')\n",
    "    plt.plot(x,average_score,label = 'average reward')\n",
    "    plt.axhline(y=sum(average_score)/len(average_score),c='r', ls=\"--\")\n",
    "    plt.axhline(y=0,c='g', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-9, 1))\n",
    "    plt.text(1, -8, time_2 - time_1 )\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'.jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(x,all_variance,label = 'all_variance')\n",
    "    plt.axhline(y=sum(all_variance)/len(all_variance),c='r', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.title(filename + \"_var\")\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'_var.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "    print(filename)\n",
    "    torch.save(policy, filename+'.pth')  \n",
    "    print(\"time:\", time_1 - time_2)\n",
    "\n",
    "    ''' save training curve in file '''\n",
    "    reward_name = filename + \"-rew.csv\"\n",
    "    file = open(reward_name, 'w')\n",
    "    s = \"\\n\".join([str(x) for x in average_score])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    # file.write(str(time_2 - time_1))    # save time\n",
    "    file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    ''' save variance in file '''\n",
    "    var_name = filename + \"-var.csv\"\n",
    "    file = open(var_name, 'w')\n",
    "    # file.write(\"varience\\n\")    # save time\n",
    "    b = \"\\n\".join([str(x) for x in all_variance])\n",
    "    file.write(b)\n",
    "    if isContiStateValue:\n",
    "        file.write(\"statevalue\")    # save if used state value as baseline\n",
    "        file.write(\"\\n\")\n",
    "        s = \";\".join([str(x) for x in average_state_value]) # save the result of state value \n",
    "        file.write(s)\n",
    "    file.close()\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seed in range(0,11):\n",
    "#     conti(seed, n_episodes = 5000, state_value=False, white=True, number=0, lr_n=3, h_size_policy=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN suitable for differentiable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out,size_hidden):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\tself.layer1 = nn.Linear(size_in, size_hidden)\n",
    "\t\tself.layer2 = nn.Linear(size_hidden, size_out)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "\t\tm = nn.Tanh()\n",
    "\t\tactivation1 = F.relu(self.layer1(obs))\n",
    "\t\toutput = 2 * m(self.layer2(activation1))\t# the mean value\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train REINFORCE with differentiable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conti_mean_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from han_pendulum2 import Han_Pendulum2 # differentiable model\n",
    "def conti_mean_model(ns, n_episodes = 5000, state_value = False, white = False, number = 0, lr_n = 2, lr_baseline = 1, h_size_policy = 64, h_size_baseline = 64,\\\n",
    "    learning_rate_policy = None, learning_rate_baseline = None, prefix = \"\", method = 0):\n",
    "    \"\"\"train REINFORCE with differentiable model\n",
    "\n",
    "    Args:\n",
    "        ns (int): number of random seed\n",
    "        n_episodes (int, optional): number of episodes. Defaults to 5000.\n",
    "        state_value (bool, optional): if use baseline technique state value. Defaults to False.\n",
    "        white (bool, optional): if use whitening technique. Defaults to False.\n",
    "        number (int, optional): only for distinguish filename. Defaults to 0.\n",
    "        lr_n (int, optional): learning rate of optimizer 1e-n. Defaults to 2.\n",
    "        lr_baseline (int, optional): learning rate of baseline statevalue, 1e-n. Defaults to 1.\n",
    "        h_size_policy (int, optional): size of hidden layer of policy. Defaults to 64.\n",
    "        h_size_baseline (int, optional): size of hidden layer of baseline statevalue. Defaults to 64.\n",
    "        learning_rate_baseline (_type_, optional): specfic learning rate for baseline state value, not 1e-n, but the whole value. Defaults to None.\n",
    "        prefix (str, optional): prefix of filename. Defaults to \"\".\n",
    "        method (int, optional): use which function to calculate gradient. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    ''' configuration '''\n",
    "    time_1 = time.time()\n",
    "    n_episodes = n_episodes\n",
    "    max_t = 200\n",
    "    gamma = 0.9\n",
    "    h_size = h_size_policy\n",
    "\n",
    "    lr = 10**(-lr_n)\n",
    "    conti_baseline_lr = 10**(-lr_baseline)\n",
    "\n",
    "    if learning_rate_policy != None:\n",
    "        lr = learning_rate_policy\n",
    "        lr_n = learning_rate_policy\n",
    "    if learning_rate_baseline != None:\n",
    "        conti_baseline_lr = learning_rate_baseline\n",
    "        lr_baseline = learning_rate_baseline\n",
    "\n",
    "    np.random.seed(ns)\n",
    "    torch.manual_seed(ns) # set random seed\n",
    "    random.seed(ns)\n",
    "    conti_state_value_hsize = h_size_baseline\n",
    "    isContiStateValue = state_value\n",
    "    whitening = white\n",
    "\n",
    "\n",
    "    filename = f\"{n_episodes}-{max_t}-{lr_n}-hs{h_size}-\"\n",
    "    prefix = prefix + \"_\"\n",
    "    filename = \"R-tanh-\" + f\"{method}_\" + prefix + filename\n",
    "\n",
    "    if whitening:\n",
    "        filename = filename + \"-wh\"\n",
    "\n",
    "    filename = filename + f\"_sd{ns}_\" + str(number)\n",
    "    print(filename)\n",
    "    policy = Network(size_hidden=h_size,size_in=3,size_out=1)\n",
    "    \n",
    "    if isContiStateValue:    # baseline\n",
    "        conti_state_value_policy = StateValue(h_size=conti_state_value_hsize)\n",
    "        optimizer_state_value = optim.Adam(conti_state_value_policy.parameters(), lr=conti_baseline_lr)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    ''' finished configuration '''\n",
    "\n",
    "    average_score = []\n",
    "    average_state_value = []\n",
    "    all_variance = []\n",
    "\n",
    "    ''' init env '''\n",
    "    # env = gym.make(\"Pendulum-v1\")\n",
    "    env = Han_Pendulum2(seed=ns)\n",
    "\n",
    "    ''' training phase '''\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        # print(f\"----- [{i_episode}] -----\")\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_rewards_original = []\n",
    "        episode_entropies = []\n",
    "        conti_episode_state_values = []\n",
    "        obs = env.reset()   \n",
    "\n",
    "        ''' start each step'''\n",
    "        for t in range(max_t):\n",
    "            ''' establish a normal distributaion and sample one action with rsample()'''\n",
    "            ''' also need .detach() to compare with Gym '''\n",
    "            mean = policy(obs)\n",
    "            normal = Normal(mean, 1)\n",
    "            # action = normal.sample()\n",
    "            action = normal.rsample()\n",
    "            # action = action.detach()\n",
    "            # log_prob = normal.log_prob(action)\n",
    "            log_prob = normal.log_prob(action.detach())\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            # to compare with GYM, if do not use gradient of reward\n",
    "            # obs = obs.detach()\n",
    "            # reward = reward.detach()\n",
    "            # state, reward, done, _ = env.step(action.detach().numpy())\n",
    "            # state, reward, done, _ = env.step(action.numpy())\n",
    "\n",
    "            # episode_entropies.append(entropy)\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "            episode_rewards_original.append(reward)\n",
    "            \n",
    "            # state = torch.Tensor([state])\n",
    "            ''' end each step '''\n",
    "\n",
    "        ''' save average value, before operation'''\n",
    "        average_score.append(sum(episode_rewards_original).item()/max_t)\n",
    "\n",
    "        ''' process reward with whitening '''\n",
    "        if whitening:\n",
    "            # episode_rewards = torch.tensor(episode_rewards,requires_grad=True).float()\n",
    "            episode_rewards = torch.FloatTensor(episode_rewards)\n",
    "            episode_rewards.requires_grad = True\n",
    "            all_variance.append(episode_rewards.var().item())\n",
    "            episode_rewards = (episode_rewards - episode_rewards.mean())/episode_rewards.std()\n",
    "\n",
    "        ''' update policy '''\n",
    "        R = torch.zeros(1, 1)\n",
    "        baseline = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        state_value_delta = []\n",
    "        R_list = []\n",
    "\n",
    "        for i in reversed(range(len(episode_rewards))):\n",
    "            R = gamma * R + episode_rewards[i]  # R is value-function of each step\n",
    "\n",
    "            ''' try different loss function '''\n",
    "            if method == 0:\n",
    "                loss = loss - (episode_log_probs[i]*(R.detach())).sum()\n",
    "\n",
    "            elif method == 1:\n",
    "                loss = loss - (episode_log_probs[i]*(R)).sum()\n",
    "\n",
    "            elif method == 2:\n",
    "                loss = loss - (torch.exp(episode_log_probs[i])*(R)).sum()\n",
    "\n",
    "            elif method == 3:\n",
    "                loss = loss - (R).sum()\n",
    "\n",
    "            elif method == 4:\n",
    "                loss = loss - (episode_log_probs[i]*R.detach() + R).sum()\n",
    "\n",
    "        loss = loss / len(episode_rewards)\n",
    "        # print(R_list)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(f\"Episode: {i_episode}/{n_episodes}, average:{average_score[-1]}\")\n",
    "\n",
    "    ''' save plot '''\n",
    "    time_2 = time.time()\n",
    "    x = range(1,len(average_score)+1)\n",
    "    \n",
    "    if isContiStateValue:\n",
    "        plt.plot(x,average_state_value,label = 'average state value',c='r')\n",
    "    plt.plot(x,average_score,label = 'average reward')\n",
    "    plt.axhline(y=sum(average_score)/len(average_score),c='r', ls=\"--\")\n",
    "    plt.axhline(y=0,c='g', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-9, 1))\n",
    "    plt.text(1, -8, time_2 - time_1 )\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'.jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(x,all_variance,label = 'all_variance')\n",
    "    plt.axhline(y=sum(all_variance)/len(all_variance),c='r', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.title(filename + \"_var\")\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'_var.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "    ''' save data in file '''\n",
    "    print(filename)\n",
    "    torch.save(policy, filename+'.pth')  \n",
    "    print(\"time:\", time_1 - time_2)\n",
    "    ''' save data in file '''\n",
    "    file_name = filename + \".csv\"\n",
    "    file = open(file_name, 'w')\n",
    "    s = \"\\n\".join([str(x) for x in average_score])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    # file.write(str(time_2 - time_1))    # save time\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"varience\")    # save time\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    b = \"\\n\".join([str(x) for x in all_variance])\n",
    "    file.write(b)\n",
    "    if isContiStateValue:\n",
    "        file.write(\"statevalue\")    # save if used state value as baseline\n",
    "        file.write(\"\\n\")\n",
    "        s = \";\".join([str(x) for x in average_state_value]) # save the result of state value \n",
    "        file.write(s)\n",
    "    file.close()\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc5af479086f4608decbb100b98c7101a70c8924355d1bcefce5624369fc4f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
