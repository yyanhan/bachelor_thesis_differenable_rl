{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import MultivariateNormal\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi\n",
    "\n",
    "def angle_normalize_np(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out,size_hidden):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\tself.layer1 = nn.Linear(size_in, size_hidden)\n",
    "\t\tself.layer2 = nn.Linear(size_hidden, size_hidden)\n",
    "\t\tself.layer3 = nn.Linear(size_hidden, size_out)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float)\n",
    "\t\tm = nn.Tanh()\n",
    "\t\tactivation1 = F.relu(self.layer1(obs.float()))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\t\toutput = 2 * m(output)\n",
    "\t\treturn output.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PPO with Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO(seed = 0, profix = \"\", timesteps_per_batch = 5000):\n",
    "    ''' hyperparameter '''\n",
    "    # collect data\n",
    "    timesteps_per_batch = timesteps_per_batch   # Number of timesteps to run per batch, episode\n",
    "    max_timesteps_per_episode = 200             # Max number of timesteps per episode, steps\n",
    "    total_timesteps = 200                       # collect how many times\n",
    "    n_updates_per_iteration = 1                 # Number of times to update actor/critic per iteration\n",
    "    lr = 0.005                                  # Learning rate of actor optimizer\n",
    "    gamma = 0.95                                # Discount factor to be applied when calculating Rewards-To-Go\n",
    "    clip = 0.2                                  # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "    \n",
    "    ''' init env '''\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "    ''' config filenames '''\n",
    "    profix = profix + \"_\"\n",
    "    # filename = f\"PPO_{profix}total[{total_timesteps}]_up[{n_updates_per_iteration}]_ep[{timesteps_per_batch}]_step[{max_timesteps_per_episode}]_lr[{lr}]_cp[{clip}]_sd[{seed}]-test\"\n",
    "    filename = f\"PPO_GYM_{timesteps_per_batch}_sd{seed}\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # set random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    ''' init networks '''\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]        # 1\n",
    "    \n",
    "    actor = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    critic = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "    cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "\n",
    "    all_average_reward = []\n",
    "    all_variance = []\n",
    "\n",
    "    ''' methods '''\n",
    "    def rollout():  # collect data for episode times, each episode contains n steps \n",
    "        # Batch data. For more details, check function header.\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "        batch_reward_average = []\n",
    "        reward_all = []\n",
    "\n",
    "        ep_rews = []\n",
    "\n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "\n",
    "        # Keep simulating until we've run more than or equal to specified timesteps per batch\n",
    "        while t < timesteps_per_batch:      # like episode\n",
    "            ep_rews = [] # rewards collected per episode\n",
    "\n",
    "            # Reset the environment. sNote that obs is short for observation. \n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Run an episode for a maximum of max_timesteps_per_episode timesteps\n",
    "            for ep_t in range(max_timesteps_per_episode):       # like steps\n",
    "                t += 1 # Increment timesteps ran this batch so far\n",
    "\n",
    "                # Track observations in this batch\n",
    "                batch_obs.append(obs)\n",
    "\n",
    "                # Calculate action and make a step in the env. \n",
    "                # Note that rew is short for reward.\n",
    "                action, log_prob = get_action(obs)\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "\n",
    "                # Track recent reward, action, and action log probability\n",
    "                ep_rews.append(rew)\n",
    "                reward_all.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                # If the environment tells us the episode is terminated, break\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Track episodic lengths and rewards\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_reward_average.append(sum(ep_rews)/len(ep_rews))\n",
    "\n",
    "        batch_obs = torch.from_numpy(np.array(batch_obs)\n",
    "        batch_acts = torch.from_numpy(np.array(batch_acts))\n",
    "        batch_log_probs = torch.from_numpy(np.array(batch_log_probs))\n",
    "\n",
    "        batch_rtgs = compute_rtgs(batch_rews)\n",
    "        r_all_average = sum(reward_all) / len(reward_all)\n",
    "        all_variance.append(torch.tensor(batch_rews).var().item())\n",
    "\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average\n",
    "\n",
    "    def compute_rtgs(batch_rews):\n",
    "        # The rewards-to-go (rtg) per episode per batch to return.\n",
    "        # The shape will be (num timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "\n",
    "        # Iterate through each episode\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "\n",
    "            # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "            # discounted return (think about why it would be harder starting from the beginning)\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        # Convert the rewards-to-go into a tensor\n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "        return batch_rtgs\n",
    "\n",
    "    def get_action(obs):\n",
    "        # Query the actor network for a mean action\n",
    "        mean = actor(obs)\n",
    "\n",
    "        # Create a distribution with the mean action and std from the covariance matrix above.\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "        # Sample an action from the distribution\n",
    "        action = dist.rsample()\n",
    "\n",
    "        # Calculate the log probability for that action\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        # Return the sampled action and the log probability of that action in our distribution\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "\n",
    "    def evaluate(batch_obs, batch_acts):\n",
    "        V = critic(batch_obs).squeeze()\n",
    "        mean = actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        # Return the value vector V of each observation in the batch\n",
    "        # and log probabilities log_probs of each action in the batch\n",
    "        return V, log_probs\n",
    "\n",
    "    print(f\"Learning... Running {max_timesteps_per_episode} timesteps per episode, \", end='')\n",
    "    print(f\"{timesteps_per_batch} timesteps per batch for a total of {total_timesteps} timesteps\")\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    i_so_far = 0 # Iterations ran so far\n",
    "    while t_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
    "        # Autobots, roll out (just kidding, we're collecting our batch simulations here)\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average = rollout()                     # ALG STEP 3\n",
    "\n",
    "        # all_average_reward.append(batch_rtgs.mean().item())\n",
    "        all_average_reward.append(r_all_average) \n",
    "\n",
    "        # Calculate how many timesteps we collected this batch\n",
    "        t_so_far += 1\n",
    "\n",
    "        # Increment the number of iterations\n",
    "        i_so_far += 1\n",
    "\n",
    "        # Calculate advantage at k-th iteration, V from critic, rtgs from actor\n",
    "        V, _ = evaluate(batch_obs, batch_acts)\n",
    "        A_k = batch_rtgs - V.detach()\n",
    "\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "        # This is the loop where we update our network for some n epochs\n",
    "        ''' update n times'''\n",
    "        for _ in range(n_updates_per_iteration):\n",
    "            # Calculate V_phi and pi_theta(a_t | s_t)\n",
    "            V, curr_log_probs = evaluate(batch_obs, batch_acts)\n",
    "\n",
    "            # Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
    "            ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "            # Calculate surrogate losses.\n",
    "            surr1 = ratios * A_k\n",
    "            surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "\n",
    "            # Calculate actor and critic losses.\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for actor network\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optim.step()\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for critic network\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "\n",
    "        ''' show result '''\n",
    "        if t_so_far % 100 == 0:\n",
    "            # print(f\"Episode: {t_so_far}/{total_timesteps}, average:{average_score[-1]}\")\n",
    "            print(f\"Episode: {t_so_far}/{total_timesteps}, average reward: {all_average_reward[-1]}\")\n",
    "\n",
    "    ''' save result '''\n",
    "    time_2 = time.time()\n",
    "    plt.figure(1)\n",
    "    x = range(1,len(all_average_reward)+1)\n",
    "    plt.plot(x,all_average_reward,label = 'average rewards')\n",
    "    plt.axhline(y=sum(all_average_reward)/len(all_average_reward),c='r', ls=\"--\")\n",
    "    plt.axhline(y=0,c='g', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-9, 1))\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename + '_collect_rewards' + '.jpg')\n",
    "    plt.show()\n",
    "    print(filename)\n",
    "    torch.save(actor, filename+'_actor' + '.pth')  \n",
    "    torch.save(critic, filename+'_critic' + '.pth')  \n",
    "    print(\"---\")\n",
    "    plt.plot(x,all_variance,label = 'all_variance')\n",
    "    plt.axhline(y=sum(all_variance)/len(all_variance),c='r', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.title(filename + \"_var\")\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename +'_var.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    ''' save data in file '''\n",
    "    reward_name = filename + \"-rew.csv\"\n",
    "    file = open(reward_name, 'w')\n",
    "    ### s = \";\".join([str(x) for x in average_score])\n",
    "    s = \"\\n\".join([str(x) for x in all_average_reward])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    var_name = filename + \"-var.csv\"\n",
    "    file = open(var_name, 'w')\n",
    "    # file.write(\"varience\\n\")    # save time\n",
    "    b = \"\\n\".join([str(x) for x in all_variance])\n",
    "    file.write(b)\n",
    "\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training PPO Gym \n",
    "for i in range(0,11):\n",
    "    PPO(seed=i, profix=\"r-tanh\", timesteps_per_batch = 3000)\n",
    "    PPO(seed=i, profix=\"r-tanh\", timesteps_per_batch = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PPO with Differentiable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from han_pendulum2 import Han_Pendulum2\n",
    "\n",
    "def PPO_ts(seed = 0, profix = \"\", timesteps_per_batch = 5000, method = 0):\n",
    "    ''' hyperparameter '''\n",
    "    timesteps_per_batch = timesteps_per_batch   # Number of timesteps to run per batch, episode\n",
    "    max_timesteps_per_episode = 200             # Max number of timesteps per episode, steps\n",
    "    total_timesteps = 200                       # collect how many times\n",
    "    n_updates_per_iteration = 1                 # Number of times to update actor/critic per iteration\n",
    "    lr = 0.005                                  # Learning rate of actor optimizer\n",
    "    gamma = 0.95                                # Discount factor to be applied when calculating Rewards-To-Go\n",
    "    clip = 0.2                                  # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "    \n",
    "    ''' init env'''\n",
    "    env = Han_Pendulum2(seed=seed)\n",
    "\n",
    "    ''' config filename'''\n",
    "    profix = profix + \"_\"\n",
    "    today = datetime.datetime.now().strftime('%m%d-%H%M')\n",
    "    filename = f\"PPOtanh_{profix}md{method}_total[{total_timesteps}]_up[{n_updates_per_iteration}]_ep[{timesteps_per_batch}]_step[{max_timesteps_per_episode}]_lr[{lr}]_cp[{clip}]_sd[{seed}]\"#_{str(today)}\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # set random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    ''' init networks '''\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]        # 1\n",
    "    \n",
    "    actor = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    critic = Network(size_hidden=32, size_in=3,size_out=1)\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=lr)\n",
    "    cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "\n",
    "    all_average_reward = []\n",
    "    all_variance = []\n",
    "    \n",
    "\n",
    "    ''' methods '''\n",
    "    def rollout():  # collect data for episode times, each episode contains n steps \n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "        batch_reward_average = []\n",
    "        reward_all = []\n",
    "\n",
    "        ep_rews = []\n",
    "\n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "\n",
    "        while t < timesteps_per_batch:      # like episode\n",
    "            ep_rews = [] # rewards collected per episode\n",
    "\n",
    "            obs = env.reset()                                              \n",
    "            state = torch.tensor([env.state[0], env.state[1]], dtype=torch.float)\n",
    "            obs = torch.stack([torch.cos(state[0]), torch.sin(state[0]), state[1]])\n",
    "\n",
    "            for ep_t in range(max_timesteps_per_episode):       # like steps\n",
    "                t += 1 # Increment timesteps ran this batch so far\n",
    "\n",
    "                batch_obs.append(obs)\n",
    "                action, log_prob = get_action(obs)              # log_prob has no gradient\n",
    "                obs, rew, done, _ = env.step(action)\n",
    "                # rew = rew.detach()              \n",
    "                # state = state.detach()          \n",
    "                # obs = obs.detach()\n",
    "\n",
    "                ep_rews.append(rew)\n",
    "                reward_all.append(rew.item())   \n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "            batch_reward_average.append(sum(ep_rews)/len(ep_rews))\n",
    "\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        batch_acts = torch.stack(batch_acts)#.squeeze()            # previous: numpy\n",
    "        batch_log_probs = torch.stack(batch_log_probs)\n",
    "        batch_rtgs = compute_rtgs(batch_rews)\n",
    "        all_variance.append(torch.tensor(batch_rews).var().item())\n",
    "\n",
    "        r_all_average = sum(reward_all) / len(reward_all)\n",
    "\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average\n",
    "\n",
    "    def compute_rtgs(batch_rews):   # not change gradient\n",
    "        batch_rtgs = []\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        batch_rtgs = torch.stack(batch_rtgs)\n",
    "        return batch_rtgs\n",
    "\n",
    "    def get_action(obs):    # log has no G, A: not changed\n",
    "        ''' obs has gradient? '''\n",
    "        mean = actor(obs)       \n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        # action = dist.sample()    # TODO: changed\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob.detach()\n",
    "\n",
    "    def evaluate(batch_obs, batch_acts):        # not change G\n",
    "        '''input:\n",
    "        action has no gradient?\n",
    "        obs has gradient?\n",
    "        ''' \n",
    "        V = critic(batch_obs).squeeze()\n",
    "        mean = actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        return V, log_probs\n",
    "\n",
    "    print(f\"Learning... Running {max_timesteps_per_episode} timesteps per episode, \", end='')\n",
    "    print(f\"{timesteps_per_batch} timesteps per batch for a total of {total_timesteps} timesteps\")\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    i_so_far = 0 # Iterations ran so far\n",
    "    while t_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, r_all_average = rollout()                     # ALG STEP 3\n",
    "        # all_average_reward.append(batch_rtgs.mean().item())\n",
    "        all_average_reward.append(r_all_average) \n",
    "        t_so_far += 1\n",
    "        i_so_far += 1\n",
    "        V, _ = evaluate(batch_obs.detach(), batch_acts.detach())    # TODO: obs has no Gradient, action has no Gradient\n",
    "        A_k = batch_rtgs - V.detach()                                                                       # ALG STEP 5\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "        ''' update n times'''\n",
    "        for _ in range(n_updates_per_iteration):                                                       # ALG STEP 6 & 7\n",
    "            V, curr_log_probs = evaluate(batch_obs.detach(), batch_acts.detach())\n",
    "            ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "            \n",
    "            ''' TODO: Different methods'''\n",
    "            if method == 0:\n",
    "                surr1 = ratios * A_k.detach()\n",
    "                surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k.detach()\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "\n",
    "            elif method == 1:\n",
    "                surr1 = ratios * A_k \n",
    "                surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "\n",
    "            elif method == 2:\n",
    "                surr1 = ratios * A_k.detach() + ratios.detach() * A_k \n",
    "                surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k + torch.clamp(ratios, 1 - clip, 1 + clip).detach() * A_k\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            elif method == 3:\n",
    "                surr1 = ratios.detach() * curr_log_probs * A_k.detach() + ratios.detach() * A_k  # 0407 20:57 B\n",
    "                surr2 = torch.clamp(ratios, 1 - clip, 1 + clip).detach() * curr_log_probs * A_k.detach() + A_k * torch.clamp(ratios, 1 - clip, 1 + clip).detach()\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            elif method == 4:\n",
    "                surr1 = ratios.detach() * (curr_log_probs * A_k.detach() + A_k)  # 0407 20:30 C\n",
    "                surr2 = torch.clamp(ratios, 1 - clip, 1 + clip).detach() * (curr_log_probs * A_k.detach() + A_k)\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            elif method == 5:\n",
    "                surr1 =  (torch.exp(curr_log_probs) * A_k) / batch_log_probs.detach() # D 040721:23 2310\n",
    "                actor_loss = (-surr1).mean()           \n",
    "            elif method == 6:\n",
    "                actor_loss = (-A_k).mean()           \n",
    "\n",
    "            critic_loss = nn.MSELoss()(V, batch_rtgs.detach())\n",
    "            actor_optim.zero_grad()\n",
    "            # actor_loss.backward(create_graph = True, retain_graph=True) # TODO: \n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optim.step()\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "\n",
    "        ''' show result '''\n",
    "        if t_so_far % 100 == 0:\n",
    "            print(f\"Episode: {t_so_far}/{total_timesteps}, average reward: {all_average_reward[-1]}\")\n",
    "    ''' save result '''\n",
    "    time_2 = time.time()\n",
    "    plt.figure(1)\n",
    "    x = range(1,len(all_average_reward)+1)\n",
    "\n",
    "    plt.plot(x,all_average_reward,label = 'average rewards')\n",
    "    plt.axhline(y=sum(all_average_reward)/len(all_average_reward),c='r', ls=\"--\")\n",
    "    plt.axhline(y=0,c='g', ls=\"--\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-9, 1))\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename + '_collect_rewards' + '.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(x,all_variance,label = 'variance')\n",
    "    plt.legend()\n",
    "    plt.title(filename)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.savefig(filename + '_varience' + '.jpg')\n",
    "    plt.show()\n",
    "    print(filename)\n",
    "    torch.save(actor, filename+'_actor' + '.pth')  \n",
    "    torch.save(critic, filename+'_critic' + '.pth')  \n",
    "    print(\"---\")\n",
    "\n",
    "    ''' save data in file '''\n",
    "    file_name = filename + \".csv\"\n",
    "    file = open(file_name, 'w')\n",
    "    ## s = \";\".join([str(x) for x in average_score])\n",
    "    s = \"\\n\".join([str(x) for x in all_average_reward])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    s = \"\\n\".join([str(x) for x in all_variance])\n",
    "    file.write(s)   # save value function\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train PPO differentiable many times\n",
    "for i in range(0, 11):\n",
    "    PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 1000, method=0)\n",
    "    PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 1000, method=1)\n",
    "    PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 1000, method=2)\n",
    "    PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 1000, method=3)\n",
    "    PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 1000, method=4)\n",
    "    # PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 5000, method=5)\n",
    "    # PPO_ts(seed=i, profix=\"\", timesteps_per_batch = 5000, method=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc5af479086f4608decbb100b98c7101a70c8924355d1bcefce5624369fc4f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
